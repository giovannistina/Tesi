import sys
import os
import time
import datetime
import json
from atproto import Client, models

# --- IMPORTANT: Import the feed list generated by get_top_feeds.py ---
try:
    from otherfile import myfeeduris
except ImportError:
    print("Error: 'otherfile.py' not found. Did you run 'get_top_feeds.py'?")
    sys.exit(1)

# --- CONFIGURATION ---
OUTPUT_DIR = "feed_likes_data"
# ---------------------

def get_session():
    """Reads the session string from the local file."""
    try:
        with open('session.txt', 'r') as f:
            return f.read().strip()
    except FileNotFoundError:
        return None

def init_client():
    """Initializes the Bluesky client using the saved session."""
    client = Client()
    session_string = get_session()
    if session_string:
        print('Reusing session')
        client.login(session_string=session_string)
    else:
        raise Exception("Session file not found. Run create_session.py first.")
    return client

def get_likes(client, uri, limit=None):
    """Fetches likes for a specific feed URI with pagination."""
    cursor = None
    likes = []
    
    while True:
        try:
            # Fetch likes for the specific feed generator
            data = client.app.bsky.feed.get_likes(params={'uri': uri, 'cursor': cursor, 'limit': 100})
            
            if not data.likes:
                break
            
            likes.extend(data.likes)
            
            # Check manual limit if provided
            if limit and len(likes) >= limit:
                break

            # Update cursor for pagination
            cursor = data.cursor
            if not cursor:
                break
                
        except Exception as e:
            print(f"Error fetching likes: {e}")
            break
            
    return likes

if __name__ == '__main__':
    
    # Create output directory if it doesn't exist
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        
    client = init_client()
    
    print(f"Found {len(myfeeduris)} feeds to process from otherfile.py")
    
    # Iterate over the dictionary imported from otherfile.py
    for name, uri in myfeeduris.items():
        print(f"Processing feed: {name}...")
        
        # Sanitize filename (remove illegal characters)
        safe_name = "".join([c for c in name if c.isalnum() or c in (' ', '-', '_')]).strip()
        filename = os.path.join(OUTPUT_DIR, f"{safe_name}.jsonl")
        
        # Fetch likes
        likes = get_likes(client, uri)
        print(f"  -> Found {len(likes)} likes.")
        
        # Save to JSONL file
        with open(filename, 'w', encoding='utf-8') as f:
            for like in likes:
                # Create a clean JSON record
                record = {
                    'created_at': like.created_at,
                    'indexed_at': like.indexed_at,
                    'actor_did': like.actor.did,
                    'actor_handle': like.actor.handle,
                    'feed_uri': uri,
                    'feed_name': name
                }
                f.write(json.dumps(record) + '\n')
                
    print("Done! All feed likes downloaded.")